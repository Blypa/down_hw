{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe70DCZqx8vi",
        "outputId": "57a4e32a-db72-43c6-a4fa-cfc4e8075ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import string\n",
        "from typing import List, Set\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "DATA_DIR = os.path.join('data', '.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qfEn-wenx8vm"
      },
      "outputs": [],
      "source": [
        "def read_data(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path, names=['label', 'title', 'text'])\n",
        "\n",
        "    # Add title to the text\n",
        "    df['text'] = df['title'] + '\\n' + df['text']\n",
        "\n",
        "    # Drop title as it's not gonna be used\n",
        "    df = df.drop('title', axis=1)\n",
        "\n",
        "    # Initially labels start from 1, many models work only when labels start from 0\n",
        "    df['label'] = df['label'] - 1\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = read_data(os.path.join(DATA_DIR, 'train.csv'))\n",
        "test_df = read_data(os.path.join(DATA_DIR, 'test.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qOthNzSFx8vo"
      },
      "outputs": [],
      "source": [
        "train_df['text'] = train_df['text'].apply(word_tokenize)\n",
        "test_df['text'] = test_df['text'].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dEkcW9BAx8vp"
      },
      "outputs": [],
      "source": [
        "removal_set = set(\n",
        "    stopwords.words('english') +\n",
        "    list(string.punctuation)\n",
        ")\n",
        "\n",
        "def removal_function(items: List[str], removal_set: Set[str] = removal_set) -> List[str]:\n",
        "    return [item for item in items if item not in removal_set]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AC8wdTE0x8vq"
      },
      "outputs": [],
      "source": [
        "train_df['text'] = train_df['text'].apply(removal_function)\n",
        "test_df['text'] = test_df['text'].apply(removal_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KvIUEYmRx8vs"
      },
      "outputs": [],
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def lemmatization(items: List[str], lemmatizer: nltk.stem.WordNetLemmatizer = lemmatizer) -> List[str]:\n",
        "    return [lemmatizer.lemmatize(item) for item in items]\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(lemmatization)\n",
        "test_df['text'] = test_df['text'].apply(lemmatization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVPFqjO0x8vy",
        "outputId": "86f3c7d0-a26d-4d17-f9d0-4e81ae628ad1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['IRS', 'seek', 'people', 'claim', 'refund', 'WASHINGTON', '--', 'The', 'IRS', '73', 'million', 'tax', 'refund', 'thousand', 'taxpayer', 'whose', 'check', 'returned', 'tax', 'agency', 'undeliverable']),\n",
              "       list(['White', 'Sox', 'Sign', 'Hermanson', 'Two-Year', 'Deal', 'CHICAGO', 'Sports', 'Network', 'The', 'Chicago', 'White', 'Sox', 'come', 'term', 'two-year', '5.5', 'million', 'contract', 'pitcher', 'Dustin', 'Hermanson']),\n",
              "       list(['Dollar', 'fall', '103.75', 'yen', 'remark', 'Snow', 'The', 'dollar', 'slipped', 'close', 'lowest', 'level', 'year', 'Thursday', 'Tokyo', 'entering', '103', 'yen', 'territory', 'market', 'player', 'dumped', 'currency', 'belief', 'United', 'States', 'tacitly', 'approves', 'weakening']),\n",
              "       list(['Israel', 'accused', 'Syria', 'blast', 'Syria', 'blamed', 'Israel', 'car', 'bomb', 'exploded', 'capital', 'Damascus', 'wounding', 'three', 'people', 'The', 'bomb', 'went', 'soon', 'Palestinian', 'man', 'got', 'car', 'wife', 'daughter']),\n",
              "       list(['Virgin', 'Group', 'launch', 'US', 'online', 'music', 'service', 'LOS', 'ANGELES', 'Richard', 'Branson', '39', 'Virgin', 'Group', 'said', 'jumping', 'online', 'music', 'quot', 'Digital', 'Megastore', 'quot', 'US', 'customer', 'offering', 'song', 'downloads', '99', 'cent']),\n",
              "       list(['Christian', 'Conservatives', 'Press', 'Issues', 'Statehouses', 'Energized', 'electoral', 'win', 'conservative', 'Christians', 'pushing', 'ahead', 'state', 'local', 'initiative', 'controversial', 'issue']),\n",
              "       list(['This', 'weekend', 'TV', 'radio', 'TODAY', 'AUTO', 'RACING', '2:30', 'p.m.', '--', 'NASCAR', 'Busch', 'Series', 'Mr.', 'Goodcents', '300', 'qualifying', 'Kansas', 'City', 'Kan.', 'Speed', 'Channel', '4:10', 'p.m.', '--', 'NASCAR', 'Nextel', 'Cup', 'Banquet', '400', 'qualifying', 'Kansas', 'City', 'Kan.', 'Speed', 'Channel', '1', 'a.m.', '--', 'Japanese', 'Grand', 'Prix', 'qualifying', 'Suzuka', 'Speed', 'Channel', 'PRO', 'BASEBALL', '4', 'p.m.', '--', 'AL', 'Division', 'Series', 'Red', 'Sox', 'vs.', 'Anaheim', 'Fenway', 'Park', '...']),\n",
              "       list(['Savage', 'Is', 'A', '39', 'Drama', 'Queen', '39', 'Sanchez', 'The', 'Wales', 'midfielder', 'claimed', 'challenge', 'Michael', 'Hughes', 'World', 'Cup', 'qualifier', 'Wednesday', 'career-threatening']),\n",
              "       list(['Robinson', 'thrilled', 'England', 'captaincy', 'JASON', 'Robinson', 'settled', 'life', 'England', 'captain', 'admitted', 'hesitation', 'accepting', 'post', 'Robinson', '39', '39', 'fantastic', 'job', '39', 'leading', 'Sale', 'Sharks', 'Zurich', 'Premiership', 'summit', 'also', 'revealed']),\n",
              "       list(['10', 'die', 'attack', 'US', 'convoy', 'A', 'car', 'carrying', 'explosive', 'ripped', 'US', 'convoy', 'yesterday', 'northern', 'Iraq', 'killing', 'least', '10', 'people', 'US', 'troop', 'fought', 'persistent', 'pocket'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Some examples\n",
        "train_df.sample(10)['text'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIzXEJV0x8v1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq94iC27x8v1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKVWJ2u3x8v2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "down_hw",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "487aad33fae9066ce8bbed684dbb48116f623221015c62143f8b9fecefd8f590"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}